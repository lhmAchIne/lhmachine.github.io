---
title: (一)监督学习与线性回归
date: 2017-04-21
categories:
- 机器学习/斯坦福系列课程
tags: 
- 监督学习
- 分类
- 线性回归
- 代价函数
---



![](http://i.imgur.com/umjwhyr.png)
<center>图1 学习过程示意图</center>

对于因变量y值类型不同，可对学习问题分为两类：
（1）**回归问题**，此时所求的y值为**连续的**；
（2）**分类问题**，此时所求的y值是**离散的**；

##线性回归
​	当自变量为一个向量，即$\vec x=(x_1,x_2,...,x_n)^T$时，该投射函数如下：
$$ h(x)=h_\theta(x)=\sum_{i=0}^n\theta_ix_i=\vec \theta^T\vec x $$

​	式中，$\vec \theta$是表示$\vec x$投射到y的线性函数空间。同时约定$x_0=1$为截距。

​	学习问题即可归结为如何选取参数$\vec \theta$，一个合理的办法就是至少在已有的训练样本下，让$h(x)$即$h_\theta(x)$尽可能的接近于y。

​	为此，我们可以定义如下的**代价函数**：

$$ J(\theta)=\frac12\sum_{i=1}^m(h_\theta(\vec x^{(i)})-y^{(i)})^2 $$

​	其中m为样本数，而$(\vec x^{(i)},y^{(i)})$为第$i$个训练样本中的自变量和因变量。该代价函数用于测量每一个$\vec \theta$值下$h_\theta(\vec x^{(i)})$与对应$y^{(i)}$的接近程度。

​	则在学习问题中所求的$\vec \theta$值使得该代价函数取最小。