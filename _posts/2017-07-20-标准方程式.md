---
title: (三)标准方程式
date: 2017-07-20
categories:
- 机器学习/斯坦福系列课程
tags:
- 标准方程式
- 矩阵导数
- 最小二乘法
---

本文以Andrew Ng斯坦福机器学习课程CS229和讲义为基础，翻译总结。

在梯度下降中我们利用了一个迭代算法来最小化$J$。而本节所述方法利用$J$关于$\theta$的导数并设为0来使$J$最小化。我们将先引入一些符号方便做矩阵计算。



## 1.矩阵导数

引入函数$f: \Bbb R^{m*n}\mapsto \Bbb R$，表示从$m*n$矩阵到实数的映射。定义$f$关于A的导数如下：

$$ \nabla_A f(A) =\left[ \begin{matrix} \frac{\partial f}{\partial A_{11}} & \cdots &  \frac{\partial f}{\partial A_{1n}} \\ \vdots & \ddots & \vdots \\ \frac{\partial f}{\partial A_{m1}} & \cdots & \frac{\partial f}{\partial A_{mn}} \end{matrix} \right] $$

由上式可知，$\nabla_A f(A)$是一个自身的m*n的矩阵，此矩阵的$(i,j)$元素为$\frac{\partial f}{\partial A_{ij}}$。

例如，假设$A=\left[\begin{matrix}  \frac{\partial f}{\partial A_{11}} & \frac{\partial f}{\partial A_{12}} \\ \frac{\partial f}{\partial A_{21}} & \frac{\partial f}{\partial A_{22}} \end{matrix} \right]$，为一个2x2的矩阵，函数$f: \Bbb R^{2*2} \mapsto \Bbb R$如下式所示：

$$ f(A)=\frac32A_{11}+5A_{12}^2+A_{21}A_{22} $$

此处，$A_{ij}$表示矩阵A的第$(i,j)$元素，可得：

$$ \nabla_Af(A)=\left[\begin{matrix} \frac32 & 10A_{12} \\ A_{22} & A_{21} \end{matrix} \right] $$

同时，我们引入**迹**，记为tr。假设有一个n阶方阵A，则A的迹定义为它的对角线上元素的总和，即：$tr A=\sum_{i=1}^nA_{ii}$。假设有一个实数$a$，则$tr a=a$。

对矩阵AB，其迹有如下的性质：若AB是方阵，则$tr AB = tr BA$。并可得到如下推论：

$$ \lbrace \begin{array} ttrABC= trBCA = trCAB \\ trABCD = trBCDA = trCDAB = trDABC \end{array} $$

则，可易证得：$tr A = A^T, tr(A+B)=trA+trB, tr aA=atrA$，其中A，B都是方阵，$a$为一个实数。

 现在我们给出如下的公式：

$$ \nabla_AtrAB=B^T $$---:(1)

$$ \nabla_{A^T}f(A)=(\nabla_Af(A))^T $$---:(2)

$$ \nabla_A trABA^TC=CAB+C^TAB^T $$---:(3)

$$ \nabla_A|A|=|A|(A^{-1})^T $$---:(4)

其中等式(4)只应用于无奇点方阵A，|A|代表A的行列式。

观察等式(1)，假设$B\in R^{n*m}$，可根据$f(A)=trAB$定义映射函数$f:\Bbb R^{m*n}\mapsto \Bbb R$，可见我们对于矩阵导数的定义$\nabla_Af(A)$结果为一个$m*n$矩阵，而这个矩阵中第(i,j)元素为$B^T$中的第(i,j)元或是$B_{ji}$。注意，此式成立仅当$A\in \Bbb R^{m*n}$，此时AB为一个方阵，才可以使用迹操作。



## 2.最小二乘法修正



利用矩阵导数的概念，我们可以求得闭合形式下的$\theta$值使得$J(\theta)$最小。

给定一个训练集，定义**设计矩阵**X，其包含所有训练集的输入自变量，并以行排列形式存储。则X的大小应为$m*n$，实际应为$m*(n+1)$，因为每个训练样本中存在截距$x_0$，则X如下：

$$ X=\left[\begin{matrix} (\vec x^{(1)})^T \\ \vdots \\ (\vec x^{(m)})^T \end{matrix} \right] $$

同时$\vec y$为m维包含训练集中因变量y的向量，则：

$$ \vec y = \left[\begin{matrix} y^{(1)} \\  \vdots \\ y^{(m)} \end{matrix}\right] $$

由于$h_\theta(\vec x^{(i)})=(\vec x^{(i)})^T\vec \theta$，可证得：

$$ X\vec \theta-\vec y=\left[\begin{matrix} (\vec x^{(1)})^T\vec \theta \\ \vdots \\ (\vec x^{(m)})^T\vec \theta \end{matrix} \right] - \left[\begin{matrix} y^{(1)} \\ \vdots \\ y^{(m)} \end{matrix} \right]=\left[\begin{matrix} h_\theta(\vec x^{(1)})-y^{(1)} \\ \vdots \\ h_\theta(\vec x^{(m)})-y^{(m)} \end{matrix} \right] $$

对于$\vec z$，有$\vec z^T\vec z=\sum_iz_i^2$，应用此公式可得：

$$ \frac12(X\vec \theta-\vec y)^T(X\vec \theta-\vec y)=\frac12\sum_{i=1}^m(h_\theta(\vec x^{(i)})-y^{(i)})^2=J(\theta) $$

另外，结合公式(2)和(3)联立可得：

$$ \nabla_{A^T}trABA^TC=B^TA^TC^T+BA^TC $$---:(5)

应用给定的5个公式，可推出$J(\theta)$关于$\theta$的导数，可得：

$$ \begin{array}\\ \nabla_\theta J(\theta)&=\nabla_\theta\frac12(X\vec \theta-\vec y)^T(X\vec \theta-\vec y) \\ &=\frac12\nabla_\theta(\vec \theta^TX^TX\vec \theta-\vec \theta^TX^T\vec y-\vec y^TX\vec \theta+\vec y^T\vec y) \\ &=\frac12\nabla_\theta tr(\vec \theta^TX^TX\vec \theta-\vec \theta^TX^T\vec y-\vec y^TX\vec \theta+\vec y^T\vec y) \\ &=\frac12\nabla_\theta(tr\vec \theta^TX^TX\vec \theta-2tr\vec y^TX\vec \theta) \\ &=\frac12(X^TX\vec \theta+X^TX\vec \theta-2X^T\vec y) \\ &=X^TX\vec \theta-X^T\vec y \end{array} $$

在第三步中，我们应用性质"*一个实数的迹仍是这个数*"。第四步，使用公式$trA=trA^T$。第五步，使用等式(5)（其中，$A^T=0, B=B^T=X^TX, C=1$）和等式(1)。

为了最小化$J$，可设其导数为0，即得标准方程式$X^TX\vec \theta=X^T\vec y$，则可得闭合形式下使得$J(\theta)$最小的$\theta$如下：

$$ \vec \theta=(X^TX)^{-1}X^T\vec y $$