---
title: (五)局部权重线性回归
date: 2017-07-27
categories:
- 机器学习/斯坦福系列课程
tags:
- LWR
- 局部权重线性回归
- 参数无关
- 权重
---



特征的选择对于一个学习算法的良好表现来说是重要的（当我们讨论模型选择时，我们视算法选择一个好的特征的集合）。本节中，我们简略地讨论下局部权重线性回归(LWR)算法。假定有足够的数据，LWR算法会让特征的选择的影响小一些。

在最初的线性回归算法中，为了在一个查询点x作预测，即求h(x)的值，我们将：

​	1.调整$\theta$使$\sum_i(y^{(i)}-\vec \theta^T\vec x^{(i)})^2$最小化；

​	2.输出$\vec \theta^T\vec x$。

相反，局部权重线性回归算法如下：

​	1.调整$\theta$使$\sum_iw^{(i)}(y^{(i)}-\vec \theta^T\vec x^{(i)})$最小化；

​	2.输出$\vec \theta^T\vec x$。

式中，$w^{(i)}$是非负的**权重值**。直观地，如果$w^{(i)}$对i的特定值是巨大的，在选择$\theta$尽量使$(y^{(i)}-\vec \theta^T\vec x^{(i)})^2$小；如果$w^{(i)}$是微小的，误差项$(y^{(i)}-\vec \theta^T\vec x^{(i)})^2$在调整中将几乎被忽略。

权重的一个相当标准的选择是：

<center>$$w^{(i)}=exp(-\frac{(x^{(i)}-x)^2}{2\tau^2})$$</center>

如果x为一个向量值，对于一个合适的$\tau$和$\sum$，通常为$w^{(i)}=exp(-\frac{(\vec x^{(i)}-\vec x)^T(\vec x^{(i)}-\vec x)}{(2\tau^2)})$或者是$w^{(i)}=exp(-\frac{(\vec x^{(i)}-\vec x)^T\sum^{-1}(\vec x^{(i)}-\vec x)}2)$。

注意，权重取决于我们要计算x的特定点x。此外，如果$|x^{(i)}-x|$是微小的，则$w^{(i)}$接近于1。如果$|x^{(i)}-x|$是巨大的，那么$w^{(i)}$很小。因此，选择的$\theta$给出了接近查询点x的训练样本(即误差)更高的权重。也注意权重公式是一种类似于高斯分布的形式，但$w^{(i)}$确实和高斯没有任何直接的关系，尤其是$w^{(i)}$不是随机变量或者正太分布或者其他的。参数$\tau$控制着一个训练样本中的$x^{(i)}$与查询点x的距离的权重下降的速度，因此也成为**带宽**参数。

​局部权重线性回归是我们即将遇到的**参数无关**算法的第一个例子，而我们之前遇到的无权重线性回归算法，有一个混合的、有限的适合数据的参数数量，因此也被称为**参数化**学习算法。一旦我们使$\theta$适合并存储，我们将不再需要保留训练数据来做未来的预测。相反，使用局部权重线性预测时，我们需要保留全部的训练集。”无参“术语粗略地指为了表示假设h与训练集大小成线性增长，我们需要保存大量数据。