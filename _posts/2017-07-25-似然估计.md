---
title: (四)似然估计
date: 2017-07-25
categories:
- 机器学习/斯坦福系列课程
tags:
- 似然估计
- 似然函数
- 对数似然
---



​	本节中，我们将给定一系列概率上的假设，在这些假设下，最小均方回归是一种非常自然的算法。

​	假设目标变量和输入值之间的关系可通过下式表示：

<center>$$y^{(i)}=\vec \theta^T \vec x^{(i)}+\epsilon^{(i)}$$</center>

​	$\epsilon^{(i)}$是误差项，意为未建模的因素（例如有一些对预测的目标变量相关的，却未计入回归中的特征）或者是随机噪声。更进一步，假设$\epsilon^{(i)}$与$(0,\sigma^2)$高斯分布为独立同分布，则可记为$\epsilon^{(i)}\thicksim \mathcal N(0,\sigma^2)$。即$\epsilon^{(i)}$的密度如下：

<center>$$\rho(\epsilon^{(i)})=\frac1{\sqrt{2\pi}\sigma}exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})$$</center>

​	即表示：

<center>$$p(y^{(i)}|\vec x^{(i)};\vec \theta)=\frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\vec \theta^T\vec x^{(i)})^2}{2\sigma^2})$$</center>

​	符号”$p(y^{(i)}|x^{(i)};\vec \theta)$“表示$y^{(i)}$在给定$\vec x^{(i)}$和参数$\vec \theta$下的分布。需要注意的是，由于$\vec \theta$不是一个随机变量，所以我们不应该以$\theta$为条件。我们也可将上式记为，$y^{(i)}|\vec x^{(i)};\vec \theta \thicksim \mathcal N(\vec \theta^T\vec x^{(i)},\sigma^2)$

​	给定设计矩阵X（包含所有的$\vec x^{(i)}$）和$\vec \theta$，则概率可以由$p(\vec y|X;\vec \theta)$给出。这个量通常可以被视为在$\theta$上的$\vec y$或者X的函数，当我们要明确地将其表示为$\vec \theta$的函数，我们称为**似然函数**：

<center>$$L(\vec \theta)=L(\vec \theta;X,\vec y)=p(\vec y|X;\vec \theta)$$</center>

​	注意，由于独立假设$\epsilon^{(i)}$（也是给定$x^{(i)}$下的$y^{(i)}$），上式可写为：

<center>$$L(\vec \theta)=\prod_{i=1}^mp(\vec y|\vec x^{(i)};\vec \theta)=\prod_{i=1}^m\frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\vec \theta^T\vec x^{(i)})^2}{2\sigma^2})$$</center>

​	现在给定这个关于$y^{(i)}$和$\vec x^{(i)}$的概率模型之后，我们可根据**最大似然**准则来选取$\vec \theta$值，其准则如下：$\vec \theta$应该让数据具有尽可能高的可能性，即$\vec \theta$应该使$L(\vec \theta)$最大。

​	我们也可以最大化任何$L(\vec \theta)$的严格增长函数，特别是，当我们使用**对数似然**$l(\vec \theta)$最大化，将使计算更加简单一点。

<center>$$\begin{array} \\l(\vec \theta) & = & logL(\vec \theta) \\ & = & log\prod_{i=1}^m\frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\vec \theta^T\vec x^{(i)})^2}{2\sigma^2}) \\ & = & \prod_{i=1}^mlog\frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\vec \theta^T\vec x^{(i)})^2}{2\sigma^2}) \\  &   = & mlog\frac1{\sqrt{2\pi}\sigma}-\frac1{\sigma^2}\bullet \frac12\sum_{i=1}^m(y^{(i)}-\vec \theta^T\vec x^{(i)})^2 \end{array}$$<center>

​	因此，最大化$l(\vec \theta)$和最小化$\frac12\sum_{i=1}^m(y^{(i)}-\vec \theta^T\vec x^{(i)})^2$（即我们最小化均方代价函数$J(\theta)$）具有相同的答案。

​	总结如下，在给定的一系列概率假设下，最小均方回归对应于寻找$\theta$的最大似然估计值，因此，最小均方回归可以被证明为一个非常自然的方法，即作最大似然估计。但是要注意，对于最小均方，概率假设决不一定要是非常完美和合理的过程，实际上也可能是其他自然的能被用来证明的假设。

​	另外需要注意的是，在我们之前的讨论中，我们最终选择的$\theta$并不取决于$\sigma^2$的值，实际上即使$\sigma^2$未知，我们也可以达到相同的结果。我们在之后讨论指数家族和广义线性模型时，会再次使用这个实际。